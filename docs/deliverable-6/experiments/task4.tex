%!TEX program = xelatex

\documentclass[11pt,titlepage]{report}
\input{../../library/import}
\input{../../library/style}
\addbibresource{../../library/bibliography.bib}

\begin{document}
The \texttt{digmod\_gui.m} program was run several times without coding with a distance of \SI{10}{\centi \meter} such that the BER was \num{0.1}. The decision threshold was discovered to be \num{0.2} for this result and $\mu=0.3$, $\sigma_n=0.05$ were left unchanged. We adjusted the number of bits to \num{500}. The program was then ran with Hamming coding and BCH coding and the BER noted. For each measurement, the time duration was also noted. The results are summarized in Table \ref{tab:task4-diff-coding}.

From the results we can observe that for each type of coding without making use of the error correcting capabilities (e.g. the first column of Table \ref{tab:task4-diff-coding}) the BER is around \num{0.1}. This is to be expected because if no error correcting mechanism is enabled, the BER should be similar regardless of the message sent. When error correction is used, the BER without coding does not change because no error correction mechanism is employed. The Hamming coding improves (lowers) the BER by a factor \num{2}. This means that about half of the errors, the errors still appearing after Hamming coding, are not due to single bit errors but must be caused by a block of errors at least two bits long because Hamming coding can all single bit errors.
For the BCH coding the mutual distance is at least \num{7}. This means that bit errors up and including length \num{2} can be corrected. This coding yields an improvement of more than a factor three, meaning more than two thirds of the original errors were due to bit errors not exceeding \num{2} bits in a \num{15} bit block.

The error correction does come with a price; time. Using no coding method, the signal is \num{5} seconds long. Using Hamming coding the signal is \num{1.75} times longer because for each block of four bits another three bits are added. For BCH the time cost is higher yet, a factor three because every five bit block is appended with ten bits of extra information.

Concluding, we can say that both Hamming code and BCH do work to improve the BER at the cost of adding redundancy. Both Hamming coding and BCH coding are more efficient than using no coding in the sense that the increase in time is less than the factor improvement in BER, which we can quantify as follows. Let the efficiency be given by the ratio between the factor between the BER before and after correction, and the factor with which the transmission time increases for a given ECC. Then,
\begin{align*}
\eta_{\text{no coding}}&=1,\\
\eta_{\text{Hamming}}&=\frac{2}{1.75}\approx1.14,\\
\eta_{\text{BCH}}&=\frac{100/4}{3}\approx8.3.
\end{align*}
This means it makes sense in this case to use Hamming or BCH coding to decrease the BER.
\begin{table}[H]
	\centering
	\caption{BER for different codings with default \texttt{digmod\_gui} settings and decision threshold of 0.2.}
	\label{tab:task4-diff-coding}
	\begin{tabular}{c c c c}
		\hline\hline
		Coding & BER (no correction) & BER (correction) & Time [s] \\
		\hline
		None & 0.1 & 0.1 & 5\\
		Hamming & 0.07 & 0.05 & 8.75 \\
		BCH & 0.116 & 0.004 & 15\\
		\hline
	\end{tabular}
\end{table}

The \texttt{digmod\_gui.m} program was run again and $\mu=0.1$ and $\sigma_n=0.3$ were set in a way that the BER with default settings and no coding was \num{0.305}. The results of the different codings are presented in Table \ref{tab:task4-diff-coding-mu-sigma}. Following the procedure above, we can resort to the efficiency calculations again. We see that
\begin{align*}
\eta_{\text{no coding}}&=1,\\
\eta_{\text{Hamming}}&=\frac{0.305/0.24}{1.75}\approx0.73,\\
\eta_{\text{BCH}}&=\frac{0.305/0.15}{3}\approx0.68.
\end{align*}
In this case it is unwise to use BCH or Hamming coding to improve the BER, likely because the noise level is too high. This means that the information sent is corrupted in such a way that the errors occur in more than \num{1} bit out of \num{7} (for Hamming coding) or \num{2} bits out of \num{15} (for BCH coding), rendering the error correction largely useless.
\begin{table}[H]
	\centering
	\caption{BER for different codings with default \texttt{digmod\_gui} settings, decision threshold of 0.2, $\mu=0.1$ and $\sigma_n=0.3$.}
	\label{tab:task4-diff-coding-mu-sigma}
	\begin{tabular}{c c c}
		\hline\hline
		Coding & BER (no correction) & BER (correction)\\
		\hline
		None & 0.305 & 0.305\\
		Hamming & 0.26 & 0.24 \\
		BCH & 0.27 & 0.15\\
		\hline
	\end{tabular}
\end{table}

Concluding, we can say that error correction is recommended when bit error probability is so that the number of corrupted bit is most of the time in the error correcting range. This means that for large bit error probilities error correction may be useless.

\end{document}